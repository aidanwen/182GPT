{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XKPK_t2l6Ck2"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "In this notebook you will be implementing a Jax version of GPT from [this](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf) paper. Please read it in order to better understand the model. In particular, pay attention to the applications of a pre-trained model to fine-tuning and few-shot learning.\n",
    "\n",
    "Afterwards, the notebook will walk you through several experiments using your pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "WactSPHN5T0T"
   },
   "outputs": [],
   "source": [
    "# basic explanation of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "SLUx-KxF78fd"
   },
   "outputs": [],
   "source": [
    "# jax explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h-jzjm1_7GSi"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qsRvOg0X8fFz",
    "outputId": "d4444bac-59e1-442b-c90c-f310df4b563f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting flax\n",
      "  Using cached flax-0.6.2-py3-none-any.whl (189 kB)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from flax) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=4.1.1 in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from flax) (4.3.0)\n",
      "Collecting jax>=0.3.16\n",
      "  Using cached jax-0.3.25-py3-none-any.whl\n",
      "Collecting rich>=11.1\n",
      "  Using cached rich-12.6.0-py3-none-any.whl (237 kB)\n",
      "Collecting tensorstore\n",
      "  Downloading tensorstore-0.1.28-cp39-cp39-macosx_10_14_x86_64.whl (9.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.2/9.2 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting optax\n",
      "  Using cached optax-0.1.4-py3-none-any.whl (154 kB)\n",
      "Requirement already satisfied: msgpack in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from flax) (1.0.3)\n",
      "Requirement already satisfied: matplotlib in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from flax) (3.5.2)\n",
      "Requirement already satisfied: numpy>=1.12 in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from flax) (1.21.5)\n",
      "Requirement already satisfied: opt-einsum in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from jax>=0.3.16->flax) (3.3.0)\n",
      "Requirement already satisfied: scipy>=1.5 in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from jax>=0.3.16->flax) (1.9.1)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from rich>=11.1->flax) (2.11.2)\n",
      "Collecting commonmark<0.10.0,>=0.9.0\n",
      "  Using cached commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from matplotlib->flax) (2.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from matplotlib->flax) (21.3)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from matplotlib->flax) (4.25.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from matplotlib->flax) (0.11.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from matplotlib->flax) (9.2.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from matplotlib->flax) (1.4.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from matplotlib->flax) (3.0.9)\n",
      "Collecting chex>=0.1.5\n",
      "  Using cached chex-0.1.5-py3-none-any.whl (85 kB)\n",
      "Collecting jaxlib>=0.1.37\n",
      "  Downloading jaxlib-0.3.25-cp39-cp39-macosx_10_14_x86_64.whl (66.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.2/66.2 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: absl-py>=0.7.1 in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from optax->flax) (1.3.0)\n",
      "Requirement already satisfied: toolz>=0.9.0 in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from chex>=0.1.5->optax->flax) (0.11.2)\n",
      "Collecting dm-tree>=0.1.5\n",
      "  Downloading dm_tree-0.1.7-cp39-cp39-macosx_10_9_x86_64.whl (109 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.5/109.5 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib->flax) (1.16.0)\n",
      "Installing collected packages: dm-tree, commonmark, tensorstore, rich, jaxlib, jax, chex, optax, flax\n",
      "Successfully installed chex-0.1.5 commonmark-0.9.1 dm-tree-0.1.7 flax-0.6.2 jax-0.3.25 jaxlib-0.3.25 optax-0.1.4 rich-12.6.0 tensorstore-0.1.28\n",
      "Requirement already satisfied: optax in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (0.1.4)\n",
      "Requirement already satisfied: absl-py>=0.7.1 in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from optax) (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from optax) (1.21.5)\n",
      "Requirement already satisfied: typing-extensions>=3.10.0 in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from optax) (4.3.0)\n",
      "Requirement already satisfied: jax>=0.1.55 in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from optax) (0.3.25)\n",
      "Requirement already satisfied: chex>=0.1.5 in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from optax) (0.1.5)\n",
      "Requirement already satisfied: jaxlib>=0.1.37 in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from optax) (0.3.25)\n",
      "Requirement already satisfied: dm-tree>=0.1.5 in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from chex>=0.1.5->optax) (0.1.7)\n",
      "Requirement already satisfied: toolz>=0.9.0 in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from chex>=0.1.5->optax) (0.11.2)\n",
      "Requirement already satisfied: scipy>=1.5 in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from jax>=0.1.55->optax) (1.9.1)\n",
      "Requirement already satisfied: opt-einsum in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from jax>=0.1.55->optax) (3.3.0)\n",
      "Requirement already satisfied: tensorflow in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (2.11.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (3.19.6)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (14.0.6)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: numpy>=1.20 in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (1.21.5)\n",
      "Requirement already satisfied: keras<2.12,>=2.11.0 in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (2.11.0)\n",
      "Requirement already satisfied: setuptools in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (63.4.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (2.11.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (1.3.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (2.1.1)\n",
      "Requirement already satisfied: tensorboard<2.12,>=2.11 in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (2.11.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (22.12.6)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: packaging in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (21.3)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (0.28.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (4.3.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (1.51.1)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (3.7.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.0.3)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.15.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.28.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from packaging->tensorflow) (3.0.9)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (5.2.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2022.9.24)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (3.3)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install flax\n",
    "!pip install optax\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (2.11.0)\n",
      "Requirement already satisfied: tensorboard<2.12,>=2.11 in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (2.11.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (2.1.1)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (3.19.6)\n",
      "Requirement already satisfied: setuptools in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (63.4.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (0.28.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (4.3.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (22.12.6)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: packaging in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (21.3)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (1.3.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (14.0.6)\n",
      "Requirement already satisfied: keras<2.12,>=2.11.0 in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (2.11.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: numpy>=1.20 in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (1.21.5)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (1.51.1)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (3.7.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (2.11.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.15.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.28.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.0.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from packaging->tensorflow) (3.0.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (5.2.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2022.9.24)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/maxvays/opt/anaconda3/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Wni6QpMz7IhW"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "This version of jaxlib was built using AVX instructions, which your CPU and/or operating system do not support. You may be able work around this issue by building jaxlib from source.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/mw/bh1rbvhd41b6d5k92k64z5ym0000gn/T/ipykernel_28644/3191593602.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# imports\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/jax/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# We want the exported object to be the class, so we first import the module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m# to make sure a later import doesn't overwrite the class.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_config_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0m_config_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/jax/config.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# TODO(phawkins): fix users of this alias and delete this file.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_src\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/jax/_src/config.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHashable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNamedTuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_src\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_src\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjax_jit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_src\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransfer_guard_lib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/jax/_src/lib/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;31m# uses instructions that are present on this machine.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjaxlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu_feature_guard\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcpu_feature_guard\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m \u001b[0mcpu_feature_guard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_cpu_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjaxlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxla_client\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mxla_client\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: This version of jaxlib was built using AVX instructions, which your CPU and/or operating system do not support. You may be able work around this issue by building jaxlib from source."
     ]
    }
   ],
   "source": [
    "# imports\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "\n",
    "import flax\n",
    "from flax import linen as nn\n",
    "# from flax.training import train_state, checkpoints\n",
    "\n",
    "import optax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "82FvhR1V9T8H"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NAW6RzUk14IL"
   },
   "source": [
    "# Helper Functions\n",
    "\n",
    "These are functions you may find helpful in your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zB6F65HU1-V4"
   },
   "outputs": [],
   "source": [
    "class TransformerGELU(nn.Module):\n",
    "    \"\"\"\n",
    "    Applies GELU function layer-wise\n",
    "    \"\"\"\n",
    "    def setup(self, approximate=False):\n",
    "        super().__init__()\n",
    "        self.approximate = approximate\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return nn.gelu(x, self.approximate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-I82fHHq9VdH"
   },
   "source": [
    "# Implementation\n",
    "\n",
    "In this section you will implement x parts of the Flax/JAX GPT model. Specifically: (list what we end up deciding)\n",
    "\n",
    "\n",
    "\n",
    "You will also be coding task-specific input transformations for fine-tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q0oa1jB_1way"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VNoBdcXw__w9"
   },
   "source": [
    "## (1) Implementing Attention and Multi-Headed Attention\n",
    "\n",
    "(Description of how GPT attention might differ from non-gpt attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l-uMA3J39Yob"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "snh5sDHBAL0b"
   },
   "source": [
    "## (2) Embedding Layer\n",
    "\n",
    "(GPT does not have positional embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mnu-RXXwAhWz"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jD7lIfccAhfw"
   },
   "source": [
    "## (3) Decoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yz11HoQEBHGS"
   },
   "outputs": [],
   "source": [
    "class TransformerDecoderBlock(nn.Module):\n",
    "    \"\"\"A decoding block from the paper Attention Is All You Need (https://arxiv.org/pdf/1706.03762.pdf).\n",
    "    :param inputs: Tensor of decoder_inputs\n",
    "\\                    decoder_inputs -> a Tensor with shape [batch_size, decoding_sequence_length, channels]\n",
    "    :return: output: Tensor with same shape as decoder_inputs\n",
    "    \"\"\"\n",
    "    input_size : int\n",
    "    n_heads : int\n",
    "    filter_size : int\n",
    "    hidden_size : int\n",
    "    dropout : float\n",
    "    def setup(self):\n",
    "        self.norm_1 = nn.LayerNorm(self.input_size)\n",
    "        self.attention = MultiHeadAttention(self.n_heads, self.input_size)\n",
    "        self.norm_2 = nn.LayerNorm(self.input_size)\n",
    "        self.feed_forward = TransformerFeedForward(self.input_size, self.filter_size, self.hidden_size, self.dropout)\n",
    "\n",
    "    def __call__(self, inputs, self_attention_mask=None):\n",
    "        norm_inputs = self.norm_1(inputs)\n",
    "        attention = self.attention(norm_inputs, mask=self_attention_mask)\n",
    "        res_attention = attention + inputs\n",
    "        output = res_attention + self.feed_forward(self.norm_2(res_attention))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2gN0iexDBHOm"
   },
   "source": [
    "## (4) Putting it all together: Transformer Decoder and GPT\n",
    "\n",
    "We have implemented the TransformerFeedForward class for you. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nWi1l5VIBpIs"
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.8.9 64-bit' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# transformer decoder\n",
    "class TransformerDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "        Stack of TransformerDecoderBlocks. Performs initial embedding to d_model dimensions, then repeated self-attention\n",
    "        followed by attention on source sequence. Defaults to 6 layers of self-attention.\n",
    "    \"\"\"\n",
    "    # embed_size,\n",
    "    # vocab_size,\n",
    "    # # output_layer,\n",
    "    # n_layers = 6,\n",
    "    # n_heads = 8,\n",
    "    # d_model = 512,\n",
    "    # d_filter = 2048,\n",
    "    # dropout = 0.1\n",
    "    embed_size : int\n",
    "    vocab_size : int\n",
    "    n_layers : int\n",
    "    n_heads : int\n",
    "    d_model : int\n",
    "    d_filter : int\n",
    "    dropout : float\n",
    "    def setup(self):\n",
    "\n",
    "        self.token_embedding = nn.Embed(self.vocab_size, self.embed_size)\n",
    "        self.pos_embedding = nn.Embed(self.d_model, self.embed_size)\n",
    "        # self.pos_embedding = nn.Embed(d_model, self.embed_size)\n",
    "\n",
    "        self.output_layer = nn.Dense(self.vocab_size, use_bias=False)\n",
    "\n",
    "        decoding_stack = [0]*self.n_layers\n",
    "        for i in range(self.n_layers):\n",
    "            decoder = TransformerDecoderBlock(self.embed_size, self.n_heads, self.d_filter, self.d_model, self.dropout)\n",
    "            setattr(self,f\"decoder{i}\",decoder)\n",
    "            decoding_stack[i] = decoder\n",
    "        # self.output_layer = output_layer\n",
    "        self.decoding_stack = decoding_stack\n",
    "        self.attention_mask = jnp.reshape(jnp.tril(jnp.ones((self.d_model, self.d_model))), (1,1,self.d_model,self.d_model))\n",
    "        self.norm = nn.LayerNorm(self.embed_size)\n",
    "        self.drop = nn.Dropout(self.dropout, deterministic=False)\n",
    "\n",
    "    # Self attention mask is a upper triangular mask to prevent attending to future targets + a padding mask\n",
    "    # attention mask is just the padding mask\n",
    "    def __call__(self, input, fine_tune = False, train=True):\n",
    "        \"\"\"\n",
    "            Args:\n",
    "                inputs: a tuple of (encoder_output, target_embedding)\n",
    "                    encoder_output: a float32 Tensor with shape [batch_size, sequence_length, d_model]\n",
    "                    target_input: either a int32 or float32 Tensor with shape [batch_size, target_length, ndims]\n",
    "                    cache: Used for fast decoding, a dictionary of tf.TensorArray. None during training.\n",
    "                mask_future: a boolean for whether to mask future states in target self attention\n",
    "            Returns:\n",
    "                a tuple of (embedding_output, output)\n",
    "                    output: a Tensor with shape [batch_size, sequence_length, d_model]\n",
    "        \"\"\"\n",
    "        seq_len = len(input)\n",
    "\n",
    "        pos = jnp.expand_dims(jnp.arange(0, stop=seq_len),0)\n",
    "\n",
    "        tok_embed = self.token_embedding(input) # (batch_size, sequence_length, d_model)\n",
    "        pos_embed = self.pos_embedding(pos) # (1, sequence_length, d_model)\n",
    "\n",
    "        decoder_output = self.drop(tok_embed + pos_embed)\n",
    "\n",
    "        self_attention_mask = (self.attention_mask[:,:,:seq_len,:seq_len] == 0)\n",
    "\n",
    "        for decoder in self.decoding_stack:\n",
    "            decoder_output = decoder(decoder_output, self_attention_mask = self_attention_mask)\n",
    "\n",
    "        decoder_output = self.norm(decoder_output)\n",
    "\n",
    "        embedding_output = self.token_embedding.attend(decoder_output)\n",
    "        output = None\n",
    "        if fine_tune:\n",
    "            output = self.output_layer(decoder_output)\n",
    "\n",
    "        return embedding_output, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5O_KxBY-Dw4S"
   },
   "outputs": [],
   "source": [
    "# gpt block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x3buWu9yDynh"
   },
   "outputs": [],
   "source": [
    "# pretrain OR import pretrained weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2jWC24WmDm1L"
   },
   "source": [
    "## (5) Task-specific Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mRIi0JWfAK2f"
   },
   "outputs": [],
   "source": [
    "# import a test task\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K2PEHv8j-XV8"
   },
   "source": [
    "# Experiments\n",
    "\n",
    "In this section you will (train) and evaluate models with different pre-training strategies. (Note: if neccessary, we could reduce the number of parameters for this part)\n",
    "\n",
    "These models are:\n",
    "(1) No unsupervised pretraining, only fine-tuning\n",
    "(2) Pretraining on same dataset as fine-tune task\n",
    "(3) Pretraining on dataset which combines data from several tasks\n",
    "(4) Pretraining on an unrelated dataset. This pretrained model is provided.\n",
    "\n",
    "Before starting, consider how you expect these models to perform (1) on their related fine-tuning task, and (2) how well these models will generalize to other tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_pretrain_batch(dataset, seq_length, batch_size):\n",
    "    indices = list(np.random.randint(0, len(dataset), size=batch_size))\n",
    "    \n",
    "    batch_input = [dataset[i:i+] for i in indices]\n",
    "    \n",
    "    return batch_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iMcZFV4jCc7c"
   },
   "outputs": [],
   "source": [
    "CHECKPOINT_PATH = \"checkpoints/\"\n",
    "import os\n",
    "\n",
    "# import default gpt model\n",
    "class TrainerModule:\n",
    "\n",
    "    def __init__(self, model_name, exmp_batch, max_iters, lr=1e-3, warmup=100, seed=42, **model_kwargs):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            model_name - Name of the model. Used for saving and checkpointing\n",
    "            exmp_batch - Example batch to the model for initialization\n",
    "            max_iters - Number of maximum iterations the model is trained for. This is needed for the CosineWarmup scheduler\n",
    "            lr - Learning rate in the optimizer\n",
    "            warmup - Number of warmup steps. Usually between 50 and 500\n",
    "            seed - Seed to use for model init\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.model_name = model_name\n",
    "        self.max_iters = max_iters\n",
    "        self.lr = lr\n",
    "        self.warmup = warmup\n",
    "        self.seed = seed\n",
    "        # Create empty model. Note: no parameters yet\n",
    "        self.model = TransformerDecoder(**model_kwargs)\n",
    "        # Prepare logging\n",
    "        self.log_dir = os.path.join(CHECKPOINT_PATH, self.model_name)\n",
    "        # Create jitted training and eval functions\n",
    "        self.create_functions()\n",
    "        # Initialize model\n",
    "        self.init_model(exmp_batch)\n",
    "\n",
    "    def batch_to_input(self, exmp_batch):\n",
    "        # Map batch to input data to the model\n",
    "        # To be implemented in a task specific sub-class\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get_loss_function(self):\n",
    "        # Return a function that calculates the loss for a batch\n",
    "        # To be implemented in a task specific sub-class\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def create_functions(self):\n",
    "        # Create jitted train and eval functions\n",
    "        calculate_loss = self.get_loss_function()\n",
    "\n",
    "        # Training function\n",
    "        def train_step(state, rng, batch):\n",
    "            loss_fn = lambda params: calculate_loss(params, rng, batch, train=True)\n",
    "            ret, grads = jax.value_and_grad(loss_fn, has_aux=True)(state.params)\n",
    "            loss, acc, rng = ret[0], *ret[1]\n",
    "            state = state.apply_gradients(grads=grads)\n",
    "            return state, rng, loss, acc\n",
    "        self.train_step = jax.jit(train_step)\n",
    "\n",
    "        # Evaluation function\n",
    "        def eval_step(state, rng, batch):\n",
    "            _, (acc, rng) = calculate_loss(state.params, rng, batch, train=False)\n",
    "            return acc, rng\n",
    "        self.eval_step = jax.jit(eval_step)\n",
    "\n",
    "    def init_model(self, exmp_batch):\n",
    "        # Initialize model\n",
    "        self.rng = jax.random.PRNGKey(self.seed)\n",
    "        self.rng, init_rng, dropout_init_rng = jax.random.split(self.rng, 3)\n",
    "        exmp_input = self.batch_to_input(exmp_batch)\n",
    "        params = self.model.init({'params': init_rng, 'dropout': dropout_init_rng}, exmp_input, train=True)['params']\n",
    "        # Initialize learning rate schedule and optimizer\n",
    "        lr_schedule = optax.warmup_cosine_decay_schedule(\n",
    "            init_value=0.0,\n",
    "            peak_value=self.lr,\n",
    "            warmup_steps=self.warmup,\n",
    "            decay_steps=self.max_iters,\n",
    "            end_value=0.0\n",
    "        )\n",
    "        optimizer = optax.chain(\n",
    "            optax.clip_by_global_norm(1.0),  # Clip gradients at norm 1\n",
    "            optax.adam(lr_schedule)\n",
    "        )\n",
    "        # Initialize training state\n",
    "        self.state = train_state.TrainState.create(apply_fn=self.model.apply, params=params, tx=optimizer)\n",
    "\n",
    "    def train_model(self, train_loader, val_loader, num_epochs=500):\n",
    "        # Train model for defined number of epochs\n",
    "        best_acc = 0.0\n",
    "        for epoch_idx in tqdm(range(1, num_epochs+1)):\n",
    "            self.train_epoch(train_loader, epoch=epoch_idx)\n",
    "            if epoch_idx % 5 == 0:\n",
    "                eval_acc = self.eval_model(val_loader)\n",
    "                self.logger.add_scalar('val/accuracy', eval_acc, global_step=epoch_idx)\n",
    "                if eval_acc >= best_acc:\n",
    "                    best_acc = eval_acc\n",
    "                    self.save_model(step=epoch_idx)\n",
    "                self.logger.flush()\n",
    "\n",
    "    def train_epoch(self, train_loader, epoch):\n",
    "        # Train model for one epoch, and log avg loss and accuracy\n",
    "        accs, losses = [], []\n",
    "        for batch in tqdm(train_loader, desc='Training', leave=False):\n",
    "            self.state, self.rng, loss, accuracy = self.train_step(self.state, self.rng, batch)\n",
    "            losses.append(loss)\n",
    "            accs.append(accuracy)\n",
    "        avg_loss = np.stack(jax.device_get(losses)).mean()\n",
    "        avg_acc = np.stack(jax.device_get(accs)).mean()\n",
    "        self.logger.add_scalar('train/loss', avg_loss, global_step=epoch)\n",
    "        self.logger.add_scalar('train/accuracy', avg_acc, global_step=epoch)\n",
    "\n",
    "    def eval_model(self, data_loader):\n",
    "        # Test model on all data points of a data loader and return avg accuracy\n",
    "        correct_class, count = 0, 0\n",
    "        for batch in data_loader:\n",
    "            acc, self.rng = self.eval_step(self.state, self.rng, batch)\n",
    "            correct_class += acc * batch[0].shape[0]\n",
    "            count += batch[0].shape[0]\n",
    "        eval_acc = (correct_class / count).item()\n",
    "        return eval_acc\n",
    "\n",
    "    def save_model(self, step=0):\n",
    "        # Save current model at certain training iteration\n",
    "        checkpoints.save_checkpoint(ckpt_dir=self.log_dir, target=self.state.params, step=step)\n",
    "\n",
    "    def load_model(self, pretrained=False):\n",
    "        # Load model. We use different checkpoint for the pretrained model\n",
    "        if not pretrained:\n",
    "            params = checkpoints.restore_checkpoint(ckpt_dir=self.log_dir, target=self.state.params)\n",
    "        else:\n",
    "            params = checkpoints.restore_checkpoint(ckpt_dir=os.path.join(CHECKPOINT_PATH, f'{self.model_name}.ckpt'), target=self.state.params)\n",
    "        self.state = train_state.TrainState.create(apply_fn=self.model.apply, params=params, tx=self.state.tx)\n",
    "\n",
    "    def checkpoint_exists(self):\n",
    "        # Check whether a pretrained model exist for this Transformer\n",
    "        return os.path.isfile(os.path.join(CHECKPOINT_PATH, f'{self.model_name}.ckpt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreTrainer(TrainerModule):\n",
    "    def batch_to_input(self, exmp_batch):\n",
    "        return exmp_batch['input']\n",
    "\n",
    "    def get_loss_function(self):\n",
    "        def calculate_loss(params, rng, batch, train):\n",
    "            rng, dropout_apply_rng = random.split(rng)\n",
    "            logits = self.model.apply({'params': params}, batch['input'],\n",
    "                                      add_positional_encoding=True,  # No positional encoding since this is a permutation equivariant task\n",
    "                                      train=train,\n",
    "                                      rngs={'dropout': dropout_apply_rng})\n",
    "            logits = logits.squeeze(axis=-1)\n",
    "            labels = batch['input'][1:]\n",
    "            loss = optax.softmax_cross_entropy_with_integer_labels(logits, labels).mean()\n",
    "            acc = (logits.argmax(axis=-1) == labels).astype(jnp.float32).mean()\n",
    "            return loss, (acc, rng)\n",
    "        return calculate_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = {'embed_size':512, 'vocab_size':10000, 'n_layers':6, 'n_heads':16, 'd_model':512, 'd_filter':2048, 'dropout':.1}\n",
    "trainer = PreTrainer(model_name='PreTrain',\n",
    "                             exmp_batch={'input':jnp.array([1,0,1,2])},\n",
    "                             max_iters=1000, **model_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "34XkhKCbAgrQ"
   },
   "source": [
    "## Experiment 1: The value of pretraining\n",
    "\n",
    "In this section we will fine-tune a randomly initialized GPT model on (task 1). We will also fine-tune the pre-trained model on the same task. \n",
    "\n",
    "Compare the results. (Which model has better performance? Which converges faster?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DKqEq2VLAJp_"
   },
   "outputs": [],
   "source": [
    "# initialize a blank GPT model\n",
    "\n",
    "# fine-tune on task 1\n",
    "\n",
    "# fine-tune pretrained model on task 1\n",
    "\n",
    "# graph results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kI2oCbgUDJzB"
   },
   "source": [
    "Q: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "50ODH2qgAxsO"
   },
   "source": [
    "## Experiment 2: Pretraining on related datasets\n",
    "\n",
    "In this section we will remove the labels from the (task 1) dataset, and use it to pretrain our GPT implementation. We will then fine-tune the model on (task 1) and (task 2), and evaluate the respective models. \n",
    "\n",
    "\n",
    "\n",
    "*   List item\n",
    "*   List item\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-XJMdPbaDMVW"
   },
   "outputs": [],
   "source": [
    "# construct dataset using a subset of (task 1) labels.\n",
    "\n",
    "# pretrain a blank GPT model on this dataset OR import the weights directly\n",
    "\n",
    "# fine-tune on (task 1) \n",
    "\n",
    "# fine-tune on (task 2)\n",
    "\n",
    "# evaluate task 1 on held-out task 1 data\n",
    "\n",
    "# evaluate task 1 on task 2 data\n",
    "\n",
    "# fine-tune for both tasks using model 4 as the pretrained model\n",
    "\n",
    "# graph results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rwEs96dkDb3t"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8q6pFPngDdML"
   },
   "source": [
    "Q: How did the model perform on (task 1)?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yACDKg8IFwmA"
   },
   "source": [
    "Now we will see how a model pretrained on multiple tasks performs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gVtVDa2SF_MV"
   },
   "outputs": [],
   "source": [
    "# pre-train using combined dataset of task 1 and 2 (model 3.1)\n",
    "\n",
    "# pre-train using combined dataste of task 1,2,3 (model 3.2)\n",
    "\n",
    "# evaluate on task 1 and task 2. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TB3eHj3uGU4D"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cxVOP-bWGXKM"
   },
   "source": [
    "Q: How did model 3.1 perform on task 1? How about model 3.2? Explain the difference in performance.\n",
    "\n",
    "Q: "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
