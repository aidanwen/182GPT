{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XKPK_t2l6Ck2"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "In this notebook you will be implementing a Jax version of GPT from [this](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf) paper. Please read it in order to better understand the model. In particular, pay attention to the applications of a pre-trained model to fine-tuning and few-shot learning.\n",
    "\n",
    "Afterwards, the notebook will walk you through several experiments using your pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WactSPHN5T0T"
   },
   "outputs": [],
   "source": [
    "# basic explanation of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SLUx-KxF78fd"
   },
   "outputs": [],
   "source": [
    "# jax explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h-jzjm1_7GSi"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qsRvOg0X8fFz",
    "outputId": "d4444bac-59e1-442b-c90c-f310df4b563f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: pip\n",
      "zsh:1: command not found: pip\n",
      "zsh:1: command not found: pip\n"
     ]
    }
   ],
   "source": [
    "!pip install flax\n",
    "!pip install optax\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for tensorflow\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Wni6QpMz7IhW"
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "\n",
    "import flax\n",
    "from flax import linen as nn\n",
    "# from flax.training import train_state, checkpoints\n",
    "\n",
    "import optax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "82FvhR1V9T8H"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NAW6RzUk14IL"
   },
   "source": [
    "# Helper Functions\n",
    "\n",
    "These are functions you may find helpful in your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "zB6F65HU1-V4"
   },
   "outputs": [],
   "source": [
    "class TransformerGELU(nn.Module):\n",
    "    \"\"\"\n",
    "    Applies GELU function layer-wise\n",
    "    \"\"\"\n",
    "    def setup(self, approximate=False):\n",
    "        super().__init__()\n",
    "        self.approximate = approximate\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return nn.gelu(x, self.approximate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-I82fHHq9VdH"
   },
   "source": [
    "# Implementation\n",
    "\n",
    "In this section you will implement x parts of the Flax/JAX GPT model. Specifically: (list what we end up deciding)\n",
    "\n",
    "\n",
    "\n",
    "You will also be coding task-specific input transformations for fine-tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q0oa1jB_1way"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VNoBdcXw__w9"
   },
   "source": [
    "## (1) Implementing Attention and Multi-Headed Attention\n",
    "\n",
    "(Description of how GPT attention might differ from non-gpt attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l-uMA3J39Yob"
   },
   "outputs": [],
   "source": [
    "# copy paste implementation here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "snh5sDHBAL0b"
   },
   "source": [
    "## (2) Embedding Layer\n",
    "\n",
    "(GPT does not have positional embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mnu-RXXwAhWz"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jD7lIfccAhfw"
   },
   "source": [
    "## (3) Decoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yz11HoQEBHGS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2gN0iexDBHOm"
   },
   "source": [
    "## (4) Putting it all together: Transformer Decoder Block and GPT\n",
    "\n",
    "We have implemented the TransformerFeedForward class for you. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nWi1l5VIBpIs"
   },
   "outputs": [],
   "source": [
    "# transformer decoder block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5O_KxBY-Dw4S"
   },
   "outputs": [],
   "source": [
    "# gpt block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x3buWu9yDynh"
   },
   "outputs": [],
   "source": [
    "# pretrain OR import pretrained weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2jWC24WmDm1L"
   },
   "source": [
    "## (5) Task-specific Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mRIi0JWfAK2f"
   },
   "outputs": [],
   "source": [
    "# import a test task\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K2PEHv8j-XV8"
   },
   "source": [
    "# Experiments\n",
    "\n",
    "In this section you will (train) and evaluate models with different pre-training strategies. (Note: if neccessary, we could reduce the number of parameters for this part)\n",
    "\n",
    "These models are:\n",
    "(1) No unsupervised pretraining, only fine-tuning\n",
    "(2) Pretraining on same dataset as fine-tune task\n",
    "(3) Pretraining on dataset which combines data from several tasks\n",
    "(4) Pretraining on an unrelated dataset. This pretrained model is provided.\n",
    "\n",
    "Before starting, consider how you expect these models to perform (1) on their related fine-tuning task, and (2) how well these models will generalize to other tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2244461737.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn [4], line 4\u001b[0;36m\u001b[0m\n\u001b[0;31m    batch_input = [dataset[i:i+] for i in indices]\u001b[0m\n\u001b[0m                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def build_pretrain_batch(dataset, seq_length, batch_size):\n",
    "    indices = list(np.random.randint(0, len(dataset), size=batch_size))\n",
    "    \n",
    "    batch_input = [dataset[i:i+] for i in indices]\n",
    "    \n",
    "    return batch_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "iMcZFV4jCc7c"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# import default gpt model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TransformerDecoder\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mTransformerPreTrainer\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msetup\u001b[39m(\u001b[38;5;28mself\u001b[39m, vocab_size, d_model, input_length, output_length, n_layers, d_filter, dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m):\n",
      "File \u001b[0;32m~/Documents/cs182/project/182GPT/transformer.py:7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mflax\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mflax\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m linen \u001b[38;5;28;01mas\u001b[39;00m nn\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mflax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtraining\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_state, checkpoints\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mattention\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MultiHeadAttention\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtransformer_utils\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/flax/training/checkpoints.py:39\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mglobal_device_array\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GlobalDeviceArray\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmultihost_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sync_global_devices\n\u001b[0;32m---> 39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m errors \u001b[38;5;28;01mas\u001b[39;00m tf_errors\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gfile  \u001b[38;5;66;03m# pytype: disable=import-error\u001b[39;00m\n\u001b[1;32m     43\u001b[0m _IMPORT_GDAM_SUCCESSFUL \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "# import default gpt model\n",
    "from transformer import TransformerDecoder\n",
    "\n",
    "class TransformerPreTrainer(nn.Module):\n",
    "    def setup(self, vocab_size, d_model, input_length, output_length, n_layers, d_filter, dropout=0, learning_rate=1e-3):\n",
    "        self.model = TransformerDecoder(vocab_size=vocab_size, d_model=d_model, n_layers=n_layers, d_filter=d_filter)\n",
    "\n",
    "        # Summarization loss\n",
    "        criterion = optax.softmax_cross_entropy_with_integer_labels()\n",
    "        self.loss_fn = lambda pred, input: criterion(pred[:-1], input[1:])\n",
    "        self.learning_rate = learning_rate\n",
    "        self.optimizer = optax.Adam(self.model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    def forward(self,batch,optimize=True):\n",
    "        pred_logits = self.model(**batch)\n",
    "        \n",
    "        loss = self.loss_fn(pred_logits,batch['input'])\n",
    "        # accuracy = (th.eq(pred_logits.argmax(dim=2,keepdim=False),target).float()*mask).sum()/mask.sum()\n",
    "        \n",
    "        if optimize:\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "                \n",
    "        return loss #, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "34XkhKCbAgrQ"
   },
   "source": [
    "## Experiment 1: The value of pretraining\n",
    "\n",
    "In this section we will fine-tune a randomly initialized GPT model on (task 1). We will also fine-tune the pre-trained model on the same task. \n",
    "\n",
    "Compare the results. (Which model has better performance? Which converges faster?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DKqEq2VLAJp_"
   },
   "outputs": [],
   "source": [
    "# initialize a blank GPT model\n",
    "\n",
    "# fine-tune on task 1\n",
    "\n",
    "# fine-tune pretrained model on task 1\n",
    "\n",
    "# graph results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kI2oCbgUDJzB"
   },
   "source": [
    "Q: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "50ODH2qgAxsO"
   },
   "source": [
    "## Experiment 2: Pretraining on related datasets\n",
    "\n",
    "In this section we will remove the labels from the (task 1) dataset, and use it to pretrain our GPT implementation. We will then fine-tune the model on (task 1) and (task 2), and evaluate the respective models. \n",
    "\n",
    "\n",
    "\n",
    "*   List item\n",
    "*   List item\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-XJMdPbaDMVW"
   },
   "outputs": [],
   "source": [
    "# construct dataset using a subset of (task 1) labels.\n",
    "\n",
    "# pretrain a blank GPT model on this dataset OR import the weights directly\n",
    "\n",
    "# fine-tune on (task 1) \n",
    "\n",
    "# fine-tune on (task 2)\n",
    "\n",
    "# evaluate task 1 on held-out task 1 data\n",
    "\n",
    "# evaluate task 1 on task 2 data\n",
    "\n",
    "# fine-tune for both tasks using model 4 as the pretrained model\n",
    "\n",
    "# graph results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rwEs96dkDb3t"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8q6pFPngDdML"
   },
   "source": [
    "Q: How did the model perform on (task 1)?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yACDKg8IFwmA"
   },
   "source": [
    "Now we will see how a model pretrained on multiple tasks performs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gVtVDa2SF_MV"
   },
   "outputs": [],
   "source": [
    "# pre-train using combined dataset of task 1 and 2 (model 3.1)\n",
    "\n",
    "# pre-train using combined dataste of task 1,2,3 (model 3.2)\n",
    "\n",
    "# evaluate on task 1 and task 2. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TB3eHj3uGU4D"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cxVOP-bWGXKM"
   },
   "source": [
    "Q: How did model 3.1 perform on task 1? How about model 3.2? Explain the difference in performance.\n",
    "\n",
    "Q: "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
