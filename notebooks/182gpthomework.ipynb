{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKPK_t2l6Ck2"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "In this notebook you will be implementing a Jax version of GPT from [this](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf) paper. Please read it in order to better understand the model. In particular, pay attention to the applications of a pre-trained model to fine-tuning and few-shot learning.\n",
        "\n",
        "Afterwards, the notebook will walk you through several experiments using your pre-trained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WactSPHN5T0T"
      },
      "outputs": [],
      "source": [
        "# basic explanation of the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SLUx-KxF78fd"
      },
      "outputs": [],
      "source": [
        "# jax explanation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-jzjm1_7GSi"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qsRvOg0X8fFz",
        "outputId": "d4444bac-59e1-442b-c90c-f310df4b563f"
      },
      "outputs": [],
      "source": [
        "!pip install flax\n",
        "!pip install optax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wni6QpMz7IhW"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random\n",
        "\n",
        "import flax\n",
        "from flax import linen as nn\n",
        "from flax.training import train_state, checkpoints\n",
        "\n",
        "import optax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82FvhR1V9T8H"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAW6RzUk14IL"
      },
      "source": [
        "# Helper Functions\n",
        "\n",
        "These are functions you may find helpful in your implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zB6F65HU1-V4"
      },
      "outputs": [],
      "source": [
        "class TransformerGELU(nn.Module):\n",
        "    \"\"\"\n",
        "    Applies GELU function layer-wise\n",
        "    \"\"\"\n",
        "    def setup(self, approximate=False):\n",
        "        super().__init__()\n",
        "        self.approximate = approximate\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return nn.gelu(x, self.approximate)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-I82fHHq9VdH"
      },
      "source": [
        "# Implementation\n",
        "\n",
        "In this section you will implement x parts of the Flax/JAX GPT model. Specifically: (list what we end up deciding)\n",
        "\n",
        "\n",
        "\n",
        "You will also be coding task-specific input transformations for fine-tuning.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q0oa1jB_1way"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNoBdcXw__w9"
      },
      "source": [
        "## (1) Implementing Attention and Multi-Headed Attention\n",
        "\n",
        "(Description of how GPT attention might differ from non-gpt attention)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l-uMA3J39Yob"
      },
      "outputs": [],
      "source": [
        "# copy paste implementation here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snh5sDHBAL0b"
      },
      "source": [
        "## (2) Embedding Layer\n",
        "\n",
        "(GPT does not have positional embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mnu-RXXwAhWz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jD7lIfccAhfw"
      },
      "source": [
        "## (3) Decoder Block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yz11HoQEBHGS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gN0iexDBHOm"
      },
      "source": [
        "## (4) Putting it all together: Transformer Decoder Block and GPT\n",
        "\n",
        "We have implemented the TransformerFeedForward class for you. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nWi1l5VIBpIs"
      },
      "outputs": [],
      "source": [
        "# transformer decoder block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5O_KxBY-Dw4S"
      },
      "outputs": [],
      "source": [
        "# gpt block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x3buWu9yDynh"
      },
      "outputs": [],
      "source": [
        "# pretrain OR import pretrained weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jWC24WmDm1L"
      },
      "source": [
        "## (5) Task-specific Head"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mRIi0JWfAK2f"
      },
      "outputs": [],
      "source": [
        "# import a test task\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2PEHv8j-XV8"
      },
      "source": [
        "# Experiments\n",
        "\n",
        "In this section you will (train) and evaluate models with different pre-training strategies. (Note: if neccessary, we could reduce the number of parameters for this part)\n",
        "\n",
        "These models are:\n",
        "(1) No unsupervised pretraining, only fine-tuning\n",
        "(2) Pretraining on same dataset as fine-tune task\n",
        "(3) Pretraining on dataset which combines data from several tasks\n",
        "(4) Pretraining on an unrelated dataset. This pretrained model is provided.\n",
        "\n",
        "Before starting, consider how you expect these models to perform (1) on their related fine-tuning task, and (2) how well these models will generalize to other tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_pretrain_batch(dataset, seq_length, batch_size):\n",
        "    indices = list(np.random.randint(0, len(dataset), size=batch_size))\n",
        "    \n",
        "    batch_input = [dataset[i:i+] for i in indices]\n",
        "    \n",
        "    return batch_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iMcZFV4jCc7c"
      },
      "outputs": [],
      "source": [
        "# import default gpt model\n",
        "from transformer import TransformerDecoder\n",
        "\n",
        "class TransformerPreTrainer(nn.Module):\n",
        "    def setup(self, vocab_size, d_model, input_length, output_length, n_layers, d_filter, dropout=0, learning_rate=1e-3):\n",
        "        self.model = TransformerDecoder(vocab_size=vocab_size, d_model=d_model, n_layers=n_layers, d_filter=d_filter)\n",
        "\n",
        "        # Summarization loss\n",
        "        criterion = optax.softmax_cross_entropy_with_integer_labels()\n",
        "        self.loss_fn = lambda pred, input: criterion(pred[:-1], input[1:])\n",
        "        self.learning_rate = learning_rate\n",
        "        self.optimizer = optax.Adam(self.model.parameters(), lr=learning_rate)\n",
        "    \n",
        "    def forward(self,batch,optimize=True):\n",
        "        pred_logits = self.model(**batch)\n",
        "        \n",
        "        loss = self.loss_fn(pred_logits,batch['input'])\n",
        "        # accuracy = (th.eq(pred_logits.argmax(dim=2,keepdim=False),target).float()*mask).sum()/mask.sum()\n",
        "        \n",
        "        if optimize:\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "                \n",
        "        return loss #, accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34XkhKCbAgrQ"
      },
      "source": [
        "## Experiment 1: The value of pretraining\n",
        "\n",
        "In this section we will fine-tune a randomly initialized GPT model on (task 1). We will also fine-tune the pre-trained model on the same task. \n",
        "\n",
        "Compare the results. (Which model has better performance? Which converges faster?)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DKqEq2VLAJp_"
      },
      "outputs": [],
      "source": [
        "# initialize a blank GPT model\n",
        "\n",
        "# fine-tune on task 1\n",
        "\n",
        "# fine-tune pretrained model on task 1\n",
        "\n",
        "# graph results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kI2oCbgUDJzB"
      },
      "source": [
        "Q: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50ODH2qgAxsO"
      },
      "source": [
        "## Experiment 2: Pretraining on related datasets\n",
        "\n",
        "In this section we will remove the labels from the (task 1) dataset, and use it to pretrain our GPT implementation. We will then fine-tune the model on (task 1) and (task 2), and evaluate the respective models. \n",
        "\n",
        "\n",
        "\n",
        "*   List item\n",
        "*   List item\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-XJMdPbaDMVW"
      },
      "outputs": [],
      "source": [
        "# construct dataset using a subset of (task 1) labels.\n",
        "\n",
        "# pretrain a blank GPT model on this dataset OR import the weights directly\n",
        "\n",
        "# fine-tune on (task 1) \n",
        "\n",
        "# fine-tune on (task 2)\n",
        "\n",
        "# evaluate task 1 on held-out task 1 data\n",
        "\n",
        "# evaluate task 1 on task 2 data\n",
        "\n",
        "# fine-tune for both tasks using model 4 as the pretrained model\n",
        "\n",
        "# graph results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rwEs96dkDb3t"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8q6pFPngDdML"
      },
      "source": [
        "Q: How did the model perform on (task 1)?  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yACDKg8IFwmA"
      },
      "source": [
        "Now we will see how a model pretrained on multiple tasks performs. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gVtVDa2SF_MV"
      },
      "outputs": [],
      "source": [
        "# pre-train using combined dataset of task 1 and 2 (model 3.1)\n",
        "\n",
        "# pre-train using combined dataste of task 1,2,3 (model 3.2)\n",
        "\n",
        "# evaluate on task 1 and task 2. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TB3eHj3uGU4D"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxVOP-bWGXKM"
      },
      "source": [
        "Q: How did model 3.1 perform on task 1? How about model 3.2? Explain the difference in performance.\n",
        "\n",
        "Q: "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
